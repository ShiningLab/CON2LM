{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e3b1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "__author__ = 'Author'\n",
    "__email__ = 'Email'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1eb57d",
   "metadata": {},
   "source": [
    "# Detecting Contradiction at the Lexical Level\n",
    "## Word Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7245f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency\n",
    "# # built-in\n",
    "import os, math, random, string\n",
    "# # public\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import nltk\n",
    "# nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from config import Config\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d8eb86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if \"DEVICE\" in os.environ:\n",
    "        return os.environ[\"DEVICE\"]\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.xpu.is_available():\n",
    "        return \"xpu\"\n",
    "    return \"cpu\"\n",
    "\n",
    "def set_random_seed(seed: int = 42):\n",
    "    \"\"\"Fix random seeds for reproducibility across Python, NumPy, and PyTorch.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "\n",
    "    # Ensures deterministic behavior where possible\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c0e48",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa80f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 0\n",
      "llm: meta-llama/Llama-3.2-3B\n",
      "CURR_PATH: ./\n",
      "RESOURCE_PATH: ./res\n",
      "DATA_PATH: ./res/data\n",
      "RESULTS_PATH: ./res/results\n",
      "LLMS_PATH: ./res/llms\n",
      "LLM_PATH: ./res/llms/meta-llama/Llama-3.2-3B\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "for k,v in config.__dict__.items():\n",
    "    print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb9515a",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dc5d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global dictionary mapping model IDs (or model families) to BOW prefixes\n",
    "BOW_PREFIX_MAP = {\n",
    "    \"meta-llama/Llama-3.2-3B\": \"Ġ\", # LLaMA 2, 3\n",
    "    \"deepseek-ai\": \"Ġ\",             # DeepSeek\n",
    "    \"EleutherAI/gpt-neo\": \"Ġ\",      # GPT-Neo\n",
    "    \"openai-community/gpt2\": \"Ġ\",   # GPT-2\n",
    "    \"facebook/opt\": \"Ġ\",            # OPT family\n",
    "    \"bigscience/bloom\": \"▁\",        # BLOOM uses SentencePiece\n",
    "    \"google/pegasus\": \"▁\",          # SentencePiece\n",
    "    \"google-t5\": \"▁\",               # T5 models\n",
    "    \"google/mt5\": \"▁\",\n",
    "    \"Salesforce/codegen\": \"Ġ\",      # CodeGen uses GPT-style\n",
    "}\n",
    "\n",
    "def get_device():\n",
    "    if \"DEVICE\" in os.environ:\n",
    "        return os.environ[\"DEVICE\"]\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    elif hasattr(torch, \"xpu\") and torch.xpu.is_available():\n",
    "        return \"xpu\"\n",
    "    return \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d7ee77",
   "metadata": {},
   "source": [
    "# I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ce0ddc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wikidata_id</th>\n",
       "      <th>country</th>\n",
       "      <th>capital</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q233</td>\n",
       "      <td>Malta</td>\n",
       "      <td>Valletta</td>\n",
       "      <td>The capital of Malta is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q262</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>Algiers</td>\n",
       "      <td>The capital of Algeria is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q889</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Kabul</td>\n",
       "      <td>The capital of Afghanistan is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q33</td>\n",
       "      <td>Finland</td>\n",
       "      <td>Helsinki</td>\n",
       "      <td>The capital of Finland is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q736</td>\n",
       "      <td>Ecuador</td>\n",
       "      <td>Quito</td>\n",
       "      <td>The capital of Ecuador is</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  wikidata_id      country   capital                         source\n",
       "0        Q233        Malta  Valletta        The capital of Malta is\n",
       "1        Q262      Algeria   Algiers      The capital of Algeria is\n",
       "2        Q889  Afghanistan     Kabul  The capital of Afghanistan is\n",
       "3         Q33      Finland  Helsinki      The capital of Finland is\n",
       "4        Q736      Ecuador     Quito      The capital of Ecuador is"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tsv\n",
    "raw_df = pd.read_csv('res/data/wiki/capital50.tsv', sep='\\t')\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ec03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = raw_df['source'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd786809",
   "metadata": {},
   "source": [
    "## Llama 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d8d7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./res/llms/meta-llama/Llama-3.2-3B\n"
     ]
    }
   ],
   "source": [
    "print(config.LLM_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c38830e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0945388bfab84377bfefc7a041dcfa3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# init model and tokenizer\n",
    "config.device = get_device()\n",
    "print(f\"Using device: {config.device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.LLM_PATH\n",
    "    , device_map=config.device\n",
    "    )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.LLM_PATH\n",
    "    , device_map=config.device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8cd79",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "23fe39a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow prefix: Ġ\n",
      "bow prefix id: 220\n"
     ]
    }
   ],
   "source": [
    "# init parameters\n",
    "top_k = 10\n",
    "beam_width = 5\n",
    "\n",
    "config.bow_prefix = BOW_PREFIX_MAP.get(config.LLM_PATH, \"Ġ\")\n",
    "config.bow_prefix_id = tokenizer.convert_tokens_to_ids(config.bow_prefix)\n",
    "print(f'bow prefix: {config.bow_prefix}')\n",
    "print(f'bow prefix id: {config.bow_prefix_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0697d541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 235892\n",
      "pathologicopsychological <|begin_of_text|>pathologicopsychological 6\n",
      "thyroparathyroidectomize <|begin_of_text|>thyroparathyroidectomize 9\n",
      "formaldehydesulphoxylate <|begin_of_text|>formaldehydesulphoxylate 10\n",
      "Estimated beam depth: 10\n"
     ]
    }
   ],
   "source": [
    "# get the max beam depth\n",
    "\n",
    "def get_english_words():\n",
    "    \"\"\"\n",
    "    Returns a list of English words sorted by character length (descending).\n",
    "    \"\"\"\n",
    "    word_list = set(words.words())\n",
    "    return sorted(word_list, key=len, reverse=True)\n",
    "\n",
    "\n",
    "def estimate_beam_depth(vocab, tokenizer, bow_prefix):\n",
    "    beam_depth = 0\n",
    "    for word in vocab[:100]:\n",
    "        token = tokenizer.encode(word, add_special_tokens=True)\n",
    "        if len(token) > beam_depth:\n",
    "            print(word, tokenizer.decode(token), len(token))\n",
    "            beam_depth = max(beam_depth, len(token))\n",
    "    return beam_depth\n",
    "\n",
    "\n",
    "vocab = get_english_words()\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "beam_depth = estimate_beam_depth(vocab, tokenizer, config.bow_prefix)\n",
    "print(f\"Estimated beam depth: {beam_depth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "81d6abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm(text_or_ids, model, tokenizer, config):\n",
    "    \"\"\"\n",
    "    Compute the next-token probability distribution for a given input.\n",
    "\n",
    "    Args:\n",
    "        text_or_ids (str or List[int]): Input string or list of token IDs\n",
    "        model: Hugging Face AutoModelForCausalLM\n",
    "        tokenizer: Corresponding tokenizer\n",
    "        config: Should contain `.device`\n",
    "\n",
    "    Returns:\n",
    "        probs (Tensor): Softmax probability distribution over vocabulary, shape [vocab_size]\n",
    "    \"\"\"\n",
    "    # Decode token IDs to text if input is not a string\n",
    "    if not isinstance(text_or_ids, str):\n",
    "        text = tokenizer.decode(text_or_ids, skip_special_tokens=True)\n",
    "    else:\n",
    "        text = text_or_ids\n",
    "\n",
    "    # Tokenize input\n",
    "    x = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = x[\"input_ids\"].to(config.device)\n",
    "    attention_mask = x[\"attention_mask\"].to(config.device)\n",
    "\n",
    "    # Run model and get logits for the last token\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits[:, -1, :]  # Last token position\n",
    "    probs = F.softmax(logits, dim=-1).squeeze()  # [vocab_size]\n",
    "\n",
    "    return probs\n",
    "\n",
    "def is_valid_token(token_id, bow_prefix, tokenizer) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the token is a valid token:\n",
    "    - start with the BOW prefix\n",
    "    - contains only alphanumeric characters\n",
    "    \"\"\"\n",
    "    token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "    token_str = tokenizer.decode(token_id)\n",
    "    return token.startswith(bow_prefix) or token_str.isalpha()\n",
    "\n",
    "def is_bow_token(token_id, bow_prefix, tokenizer) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the token is a valid token:\n",
    "    - start with the BOW prefix\n",
    "    - contains only alphanumeric characters\n",
    "    \"\"\"\n",
    "    return tokenizer.convert_ids_to_tokens(token_id).startswith(bow_prefix)\n",
    "\n",
    "def get_bow_token_ids(bow_prefix, tokenizer) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of token IDs that:\n",
    "    - start with the BOW prefix (e.g., Ġ), AND\n",
    "    - decode to alphabetic strings (i.e., isalpha())\n",
    "    \"\"\"\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    return [i for i in range(vocab_size) if is_bow_token(i, bow_prefix, tokenizer)]\n",
    "\n",
    "def is_mid_token(token_id, bow_prefix, tokenizer) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the token is a valid continuation of a word:\n",
    "    - does NOT start with the BOW prefix\n",
    "    - contains only English letters\n",
    "    \"\"\"\n",
    "    token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "    token_str = tokenizer.decode(token_id)\n",
    "    # does NOT start with the BOW prefix\n",
    "    if token.startswith(bow_prefix):\n",
    "        return False\n",
    "    # contains only English letters\n",
    "    if not token_str.isalpha():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_mid_token_ids(bow_prefix, tokenizer) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of token IDs that:\n",
    "    - start with the BOW prefix (e.g., Ġ), AND\n",
    "    - decode to alphabetic strings (i.e., isalpha())\n",
    "    \"\"\"\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    return [i for i in range(vocab_size) if is_mid_token(i, bow_prefix, tokenizer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "67824acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bow token IDs: 57875\n",
      "Mid token IDs: 41943\n",
      "Valid token IDs: 99818\n"
     ]
    }
   ],
   "source": [
    "# get valid token ids\n",
    "bow_token_ids = get_bow_token_ids(config.bow_prefix, tokenizer)\n",
    "print(f\"Bow token IDs: {len(bow_token_ids)}\")\n",
    "\n",
    "mid_token_ids = get_mid_token_ids(config.bow_prefix, tokenizer)\n",
    "print(f\"Mid token IDs: {len(mid_token_ids)}\")\n",
    "\n",
    "valid_token_ids = list(set(bow_token_ids + mid_token_ids))\n",
    "print(f\"Valid token IDs: {len(valid_token_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "219fe91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_probs(probs, valid_token_ids):\n",
    "    \"\"\"\n",
    "    Normalize probs over a precomputed set of valid token IDs.\n",
    "\n",
    "    Args:\n",
    "        probs (Tensor): Raw probability distribution over vocabulary, shape [vocab_size]\n",
    "        valid_token_ids (List[int]): Token IDs that are considered valid for normalization\n",
    "\n",
    "    Returns:\n",
    "        norm_probs (Tensor): New probability distribution normalized over valid_token_ids\n",
    "    \"\"\"\n",
    "    masked_probs = torch.zeros_like(probs)\n",
    "    masked_probs[valid_token_ids] = probs[valid_token_ids]\n",
    "    total = masked_probs.sum()\n",
    "\n",
    "    return masked_probs / total\n",
    "\n",
    "def inject_eow_prob(probs, bow_token_ids, bow_prefix_id):\n",
    "    \"\"\"\n",
    "    Inject EOW probability into the bow_prefix_id slot by reallocating \n",
    "    the total BOW mass there, and zeroing out the original BOW tokens.\n",
    "    \n",
    "    Returns a new probability tensor (not in-place).\n",
    "    \"\"\"\n",
    "    probs = probs.clone()\n",
    "    bow_mass = probs[bow_token_ids].sum()\n",
    "    probs[bow_token_ids] = 0.0\n",
    "    probs[bow_prefix_id] = bow_mass\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beam:\n",
    "    # BOW prefix ID for the model\n",
    "    bow_prefix_id = None\n",
    "    def __init__(self, token_ids, token_probs, input_ids, parent=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids (List[int]): List of token IDs generated so far\n",
    "            token_probs (List[float]): List of probabilities for each token in the beam\n",
    "            input_ids (List[int]): Full input IDs (prompt + generated tokens)\n",
    "            parent (Optional[Beam]): Parent beam for backtracking\n",
    "        \"\"\"\n",
    "        self.token_ids = token_ids\n",
    "        self.token_probs = token_probs\n",
    "        self.token_log_probs = [math.log(p) if p > 0 else float('-inf') for p in token_probs]\n",
    "        self.input_ids = input_ids\n",
    "        self.parent = parent\n",
    "\n",
    "    def extend(self, next_token_id, next_token_prob):\n",
    "        \"\"\"Return a new Beam with one more token added.\"\"\"\n",
    "        return Beam(\n",
    "            self.token_ids + [next_token_id],\n",
    "            self.token_probs + [next_token_prob],\n",
    "            self.input_ids + [next_token_id],\n",
    "            parent=self\n",
    "        )\n",
    "\n",
    "    def prob(self):\n",
    "        \"\"\"Return product of token probabilities (pseudo-probability).\"\"\"\n",
    "        return math.prod(self.token_probs) if self.token_ids else .0\n",
    "\n",
    "    def log_prob(self):\n",
    "        \"\"\"Return sum of log probabilities (more stable for ranking).\"\"\"\n",
    "        return sum(self.token_log_probs)\n",
    "\n",
    "    @property\n",
    "    def done(self):\n",
    "        \"\"\"A beam is done if the last token is a BOW token (end of word).\"\"\"\n",
    "        return self.token_ids and self.token_ids[-1] == Beam.bow_prefix_id\n",
    "\n",
    "    def path(self):\n",
    "        \"\"\"Return a list of beam nodes from root to this beam.\"\"\"\n",
    "        beam, result = self, []\n",
    "        while beam:\n",
    "            result.append(beam)\n",
    "            beam = beam.parent\n",
    "        return list(reversed(result))\n",
    "\n",
    "    def decoded(self, tokenizer):\n",
    "        \"\"\"Decode the beam's token sequence using a tokenizer.\"\"\"\n",
    "        return tokenizer.decode(self.token_ids)\n",
    "\n",
    "    def tokens(self, tokenizer):\n",
    "        \"\"\"Return a list of token strings.\"\"\"\n",
    "        return [tokenizer.decode([t]) for t in self.token_ids]\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, Beam) and self.token_ids == other.token_ids\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(self.token_ids))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Beam(tokens={self.token_ids}, prob={self.prob():.8f}, log_prob={self.log_prob():.8f})\"\n",
    "\n",
    "Beam.bow_prefix_id = config.bow_prefix_id\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a02cee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_depth = 10\n",
    "beam_width = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "126f5938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:55<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "raw_candidates = []\n",
    "for p in tqdm(prompts):\n",
    "    # Step 1: Initialize input and beam\n",
    "    input_ids = tokenizer(p, return_tensors=\"pt\").input_ids.tolist()[0]\n",
    "    beams = [Beam([], [], input_ids)]\n",
    "    # Step 2: Beam search decoding\n",
    "    for depth in range(beam_depth):\n",
    "        new_beams = []\n",
    "        for beam in beams:\n",
    "            if beam.done:\n",
    "                new_beams.append(beam)\n",
    "                continue\n",
    "            # Step 2.1: Get next-token probability distribution\n",
    "            next_probs = lm(beam.input_ids, model, tokenizer, config)\n",
    "            vocab_ids = bow_token_ids if depth == 0 else valid_token_ids\n",
    "            next_probs = norm_probs(next_probs, vocab_ids)\n",
    "            # Step 2.2: Inject end-of-word (EOW) probability at depths > 0\n",
    "            if depth:\n",
    "                next_probs = inject_eow_prob(next_probs, bow_token_ids, config.bow_prefix_id)\n",
    "            # Step 2.3: Top-k expansion\n",
    "            topk_probs, topk_ids = torch.topk(next_probs, k=beam_width)\n",
    "            for topk_id, topk_prob in zip(topk_ids.tolist(), topk_probs.tolist()):\n",
    "                new_beams.append(beam.extend(topk_id, topk_prob))\n",
    "                \n",
    "        # Step 3: Keep top-scoring beams\n",
    "        beams = sorted(new_beams, key=lambda beam: -beam.prob())[:beam_width]\n",
    "        if all(b.done for b in beams):\n",
    "            break\n",
    "    raw_candidates.append(beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e23e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-process the beams\n",
    "def post_process_beams(beams, tokenizer):\n",
    "    \"\"\"\n",
    "    Post-process the beams to extract the best candidates.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    for beam in beams:\n",
    "        decoded = beam.decoded(tokenizer)\n",
    "        prob = beam.prob()\n",
    "        tokens = tokenizer.convert_ids_to_tokens(beam.token_ids)\n",
    "        candidates.append((decoded, prob, path))\n",
    "    return candidates\n",
    "\n",
    "candidates = [post_process_beams(cs, tokenizer) for cs in raw_candidates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1b8cab97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' Valletta ',\n",
       "  0.3554948042315573,\n",
       "  [Beam(tokens=[], prob=0.00000000, log_prob=0.00000000),\n",
       "   Beam(tokens=[4196], prob=0.36004543, log_prob=-1.02152505),\n",
       "   Beam(tokens=[4196, 1169], prob=0.35597174, log_prob=-1.03290394),\n",
       "   Beam(tokens=[4196, 1169, 2629], prob=0.35590202, log_prob=-1.03309982),\n",
       "   Beam(tokens=[4196, 1169, 2629, 220], prob=0.35549480, log_prob=-1.03424465)]),\n",
       " (' the ',\n",
       "  0.12685003320938826,\n",
       "  [Beam(tokens=[], prob=0.00000000, log_prob=0.00000000),\n",
       "   Beam(tokens=[279], prob=0.12690452, log_prob=-2.06432031),\n",
       "   Beam(tokens=[279, 220], prob=0.12685003, log_prob=-2.06474973)]),\n",
       " (' Valetta ',\n",
       "  0.09286365397251106,\n",
       "  [Beam(tokens=[], prob=0.00000000, log_prob=0.00000000),\n",
       "   Beam(tokens=[27713], prob=0.09439309, log_prob=-2.36028741),\n",
       "   Beam(tokens=[27713, 1169], prob=0.09297046, log_prob=-2.37547347),\n",
       "   Beam(tokens=[27713, 1169, 2629], prob=0.09294434, log_prob=-2.37575443),\n",
       "   Beam(tokens=[27713, 1169, 2629, 220], prob=0.09286365, log_prob=-2.37662295)]),\n",
       " (' a ',\n",
       "  0.060499319170169485,\n",
       "  [Beam(tokens=[], prob=0.00000000, log_prob=0.00000000),\n",
       "   Beam(tokens=[264], prob=0.06051675, log_prob=-2.80483512),\n",
       "   Beam(tokens=[264, 220], prob=0.06049932, log_prob=-2.80512317)]),\n",
       " (' known ',\n",
       "  0.03596727310581915,\n",
       "  [Beam(tokens=[], prob=0.00000000, log_prob=0.00000000),\n",
       "   Beam(tokens=[3967], prob=0.03597192, log_prob=-3.32501672),\n",
       "   Beam(tokens=[3967, 220], prob=0.03596727, log_prob=-3.32514583)]),\n",
       " (' also ',\n",
       "  0.031308698900762266,\n",
       "  [Beam(tokens=[], prob=0.00000000, log_prob=0.00000000),\n",
       "   Beam(tokens=[1101], prob=0.03131267, log_prob=-3.46373245),\n",
       "   Beam(tokens=[1101, 220], prob=0.03130870, log_prob=-3.46385930)]),\n",
       " (' Malta ',\n",
       "  0.02631927690723712,\n",
       "  [Beam(tokens=[], prob=0.00000000, log_prob=0.00000000),\n",
       "   Beam(tokens=[61750], prob=0.02655410, log_prob=-3.62857125),\n",
       "   Beam(tokens=[61750, 220], prob=0.02631928, log_prob=-3.63745365)]),\n",
       " (' one ',\n",
       "  0.021835016232907845,\n",
       "  [Beam(tokens=[], prob=0.00000000, log_prob=0.00000000),\n",
       "   Beam(tokens=[832], prob=0.02183582, log_prob=-3.82420333),\n",
       "   Beam(tokens=[832, 220], prob=0.02183502, log_prob=-3.82424035)]),\n",
       " (' called ',\n",
       "  0.02004088719738162,\n",
       "  [Beam(tokens=[], prob=0.00000000, log_prob=0.00000000),\n",
       "   Beam(tokens=[2663], prob=0.02004764, log_prob=-3.90964391),\n",
       "   Beam(tokens=[2663, 220], prob=0.02004089, log_prob=-3.90998073)]),\n",
       " (' situated ',\n",
       "  0.015843926469235825,\n",
       "  [Beam(tokens=[], prob=0.00000000, log_prob=0.00000000),\n",
       "   Beam(tokens=[31183], prob=0.01584537, log_prob=-4.14487820),\n",
       "   Beam(tokens=[31183, 220], prob=0.01584393, log_prob=-4.14496904)])]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a460685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SenSem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
